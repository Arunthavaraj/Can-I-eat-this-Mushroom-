---
title: "Can I eat the Mushroom? - A Statistical Analysis using Classifiers."
author: "Arunthavaraj Ananthavel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    includes:
      in_header: header.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction:

The 'mushroom dataset' is a standard in machine learning, originating from a 1981 North American mushroom guide. Researchers often use this dataset to test new methods. The dataset contains information about gilled mushrooms, each described by five key features: Cap Shape, Cap Surface, Cap Color, Odor, and Height. Crucially, each mushroom is labeled as edible or poisonous. The main research question is straightforward yet critical: "Can I eat that mushroom?" The aim is to predict a mushroom's safety based on its features. To accomplish this, two common machine learning tools will be employed: Decision Trees and Random Forests. The objectives are:

- Identify which features best indicate if a mushroom is safe.
- Make accurate predictions about mushroom edibility using Decision Tree and Random Forest Classifier.
- Cross Validate the results and test the significance of the model.

This work demonstrates how machine learning can aid in making vital decisions. The findings could help people identify safe mushrooms, showcasing the importance of clear, understandable results when safety is at stake.


```{r libraries,warning=FALSE,echo=FALSE,include=FALSE,message=FALSE}
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(C50)
library(party)
library(randomForest)
library(gridExtra)
library(viridis)
library(cowplot)
library(dplyr)
```

# Data Understanding:

The mushroom dataset contains 8,124 observations, each representing a unique mushroom specimen. Every mushroom is described by six variables, all of which are categorical (non-numeric) and stored as character strings in R. This data structure is typical for datasets involving descriptive or qualitative features.

1. Target Variable:
   - Edible: This is the variable to be predicted. It has two possible values: "Poisonous" or "Edible". This makes our task a binary classification problem, which is common in many real-world scenarios where the goal is to distinguish between two distinct categories.

2. Predictor Variables:
   All predictor variables are qualitative, suggesting that the mushrooms' characteristics are best described by categories rather than numerical measurements.
   
   a. `CapShape`: Describes the shape of the mushroom's cap. Examples include "Convex" and "Bell". Different cap shapes might be indicative of different mushroom species or families.
   
   b. `CapSurface`: Describes the texture of the cap's surface. Examples include "Smooth" and "Scaly". This tactile feature could be a key identifier, as some poisonous mushrooms might have distinct surface textures.
   
   c. `CapColor`: Describes the color of the cap. Examples include "Brown", "Yellow", and "White". Color is often a critical feature in mushroom identification, as it can vary significantly between edible and poisonous species.
   
   d. `Odor`: Describes the mushroom's smell. Examples include "Pungent", "Almond", and "Anise". Odor is a fascinating feature, as many experienced foragers use smell as a key indicator. Some poisonous mushrooms are known for their distinct, often unpleasant odors.
   
   e. `Height`: A qualitative measure of the mushroom's size. It has values like "Tall" and "Short". While not as precise as a numerical measurement, this feature still provides a sense of scale, which could help distinguish between similar-looking species of different sizes.

The dataset's structure—entirely categorical—reflects the nature of mushroom identification in the field. This aligns well with the dataset's features, making it a realistic representation of how a person might assess a mushroom's edibility in a real-life scenario.

This large number of examples should allow the models to learn complex patterns and relationships between the predictors and the target variable, potentially leading to high-accuracy predictions. 

However, the categorical nature of all variables presents both opportunities and challenges. On one hand, it makes the data easily interpretable—a key goal in this project. On the other hand, it requires careful handling in machine learning models. Techniques like one-hot encoding may be needed to convert these categories into a format suitable for some algorithms. Fortunately, decision trees and random forests, the chosen methods for this task, are well-suited to handle categorical data directly.

```{r,warning=FALSE,echo=TRUE,include=TRUE}
mushrooms <- read.csv("mushrooms.csv")
print(str(mushrooms))
print(summary(mushrooms))
```
# Preliminary Analysis:

### Missing Values:

```{r missing values,warning=FALSE,echo=FALSE,include=TRUE,message=TRUE}
# Check for missing values
missing_values <- sapply(mushrooms, function(x) sum(is.na(x)))
missing_values

```

The preliminary analysis of the mushroom dataset reveals a clean and well-structured dataset with no missing values across any of the columns. The target variable, "Edible", has two unique values—`Edible` and `Poisonous` —with 'Edible' being more frequent, occurring 4,208 times. Among the predictor variables, 'Convex' is the most common cap shape (3,656 occurrences) out of six unique shapes, while 'Scaly' dominates the four cap surface textures (3,244 times). The mushrooms display a diverse range of cap colors, with ten unique options and 'Brown' being the most prevalent (2,284 times).

Interestingly, most mushrooms have no discernible odor, with 'None' being the most frequent out of nine odor types (3,528 times). Lastly, in terms of size, the dataset is skewed towards taller mushrooms, with 'Tall' appearing 4,081 times in the binary height category. This initial overview provides a solid foundation for further analysis, highlighting the distribution of key features that may influence a mushroom's edibility.

```{r,warning=FALSE,echo=FALSE,include=TRUE,message=TRUE}
# Load necessary libraries
library(knitr)
library(kableExtra)
library(dplyr)
library(tidyr)

# Load the dataset
mushrooms <- read.csv("mushrooms.csv")

# Define variables
variables <- c("Edible", "CapShape", "CapSurface", "CapColor", "Odor", "Height")

# Calculate frequency distributions and combine them
freq_combined_df <- mushrooms %>%
  select(all_of(variables)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Category") %>%
  group_by(Variable, Category) %>%
  summarise(Frequency = n(), .groups = 'drop') %>%
  arrange(Variable, desc(Frequency))

# Print the combined frequency distribution table
kable(freq_combined_df) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

### Frequency Distribution:

```{r plot distribution for each variable,warning=FALSE,echo=FALSE,include=TRUE, fig.width=12, fig.height=10,message=FALSE }

plot_distribution <- function(data, variable) {
  ggplot(data, aes_string(x = variable)) + 
    geom_bar(aes(fill = ..count..)) + 
    scale_fill_viridis(option = "D") +
    theme_minimal() +
    ggtitle(paste('Distribution of', variable)) +
    theme(
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_text(size = 12),
      axis.text.x = element_text(size = 10, angle = 90, hjust = 1),
      axis.text.y = element_text(size = 10),
      legend.position = "none"
    ) +
    labs(x = variable, y = 'Count')
}

variables <- c('Edible', 'CapShape', 'CapSurface', 'CapColor', 'Odor', 'Height')

# Generate the plots
plots <- lapply(variables, plot_distribution, data = mushrooms)
combined_plot <- plot_grid(plotlist = plots, ncol = 2, align = 'v')

ggsave("combined_plot.png", combined_plot, width = 20, height = 18, dpi = 300)

combined_plot
```



`Edible`:
The dataset is fairly balanced between edible and poisonous mushrooms, with a slightly higher count of edible mushrooms (4208) compared to poisonous mushrooms (3916). This balance suggests caution when identifying mushrooms based solely on visible features, as poisonous specimens are nearly as prevalent as edible ones.

`CapShape`:
Among the six cap shape categories, 'Convex' (3656) and 'Flat' (around 3150) are the most common, while 'Bell', 'Conical', 'Knobbed', and 'Sunken' shapes are less frequent, with counts ranging from around 300 to 600. The prevalence of certain shapes can be a key identifier in mushroom classification.

`CapSurface`: 
'Scaly' (3244) and 'Smooth' (around 3000) cap surfaces are the most common, while 'Fibrous' and 'Grooves' are also frequent but less so, with around 2100 occurrences each. The surface texture can be a crucial feature for identifying different mushroom species.

`CapColor`:
'Brown' (2284) and 'Gray' (2204) are the most common cap colors, followed closely by 'Red' (around 2000) and 'White' (around 2300). 'Green' (around 200) and 'Purple' (around 250) are the least common colors. Frequent colors like brown, gray, red, and white are crucial for accurate identification.

`Odor`:
The majority of mushrooms have either 'None' (3528) or 'Pungent' (3988) odor, while 'Spicy' odor is the least common (around 200). Other odors like 'Almond', 'Anise', 'Creosote', 'Fishy', 'Foul', and 'Musty' have moderate frequencies ranging from around 400 to 600. Odor can be an important factor in quickly identifying mushrooms.

`Height`:
The distribution between short (4043) and tall (4081) mushrooms is almost equal, suggesting that height alone is not a distinguishing factor for edibility.

Combining the frequency distribution data with earlier descriptive statistics, we can infer that the balance between edible and poisonous mushrooms demands caution in identification. Additionally, cap shape, surface texture, color, and odor appear to be crucial features for accurate classification, while height may not be as discriminating. These insights help understand the common characteristics of mushrooms in the dataset and can guide further analysis or practical identification in the field.


### Chi- Sq Test:

```{r correlation analysis using CHi sq test,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Perform Chi-squared tests
chi_squared_tests <- lapply(mushrooms[, variables], function(x) chisq.test(mushrooms$Edible, x))
names(chi_squared_tests) <- variables

# Extract Chi-squared test results
results <- lapply(chi_squared_tests, function(test) {
  data.frame(
    Statistic = as.numeric(test$statistic),
    DF = as.numeric(test$parameter),
    P_value = as.numeric(test$p.value)
  )
})

# Combine results into a single data frame
results_df <- do.call(rbind, results)
results_df$Variable <- rownames(results_df)
rownames(results_df) <- NULL
results_df <- results_df[, c("Variable", "Statistic", "DF", "P_value")]

# Identify the variable with the highest Chi-squared statistic
max_statistic_var <- results_df %>% filter(Statistic == max(Statistic)) %>% select(Variable)

# Add an association column based on p-value
results_df <- results_df %>%
  mutate(Association = ifelse(P_value < 0.05, "Strong", "Weak"))

# Add a summary row for the greatest association
summary_row <- data.frame(
  Variable = "Greatest Association",
  Statistic = NA,
  DF = NA,
  P_value = NA,
  Association = max_statistic_var$Variable
)

# Combine the summary row with the results
results_df <- rbind(results_df, summary_row)

# Print the results in a table
kable(results_df, caption = "Chi Sq test for Variables") 

```
 
###  Chi-Square Test Results:

The chi-square test examines the relationship between the 'Edible' variable and other variables. A low p-value (less than 0.05) indicates a significant relationship.

`CapShape`:
The p-value for CapShape is extremely small at 1.196457e-103. This means there is a very strong link between the mushroom cap shape and whether it is edible or poisonous. The chi-square value of 489.92 further confirms this strong association.

`CapSurface`:
The p-value for CapSurface is 5.518427e-68, also very low. This indicates a strong link between the cap surface texture and edibility. The chi-square value of 315.04 supports this finding.

`CapColor`: 
With a p-value of 6.055815e-78, CapColor shows a strong relationship with mushroom edibility. The chi-square value of 387.60 reinforces this strong association.

`Odor`:
Odor has a p-value of 0.000000e+00, effectively zero. This means there is an extremely significant link between mushroom odor and edibility. The chi-square value of 7659.73 is the highest, indicating Odor has the strongest association.

`Height`:
The p-value for Height is 4.412417e-01, much higher than 0.05. This suggests a weak relationship between mushroom height and edibility. The low chi-square value of 0.59 further confirms this weak link.

In summary, Odor, CapShape, CapSurface, and CapColor all strongly associate with determining if a mushroom is edible or poisonous. Odor stands out with the highest chi-square value. In contrast, Height has a weak, statistically insignificant relationship with edibility.


```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}

library(fmsb)
library(viridis)

# Data for the radar plot
data <- data.frame(
  Variable = c("CapShape", "CapSurface", "CapColor", "Odor", "Height"),
  Statistic = c(489.9199536, 315.0428312, 387.5977690, 7659.7267402, 0.5930512),
  DF = c(5, 3, 9, 8, 1),
  P_value = c(1.196457e-103, 5.518427e-68, 6.055815e-78, 0.000000e+00, 4.412417e-01),
  Association = c("Strong", "Strong", "Strong", "Strong", "Weak")
)

# Normalize the statistics for the radar plot (0 to 1 scale)
max_stat <- max(data$Statistic)
data_normalized <- data.frame(
  CapShape = data$Statistic[1] / max_stat,
  CapSurface = data$Statistic[2] / max_stat,
  CapColor = data$Statistic[3] / max_stat,
  Odor = data$Statistic[4] / max_stat,
  Height = data$Statistic[5] / max_stat
)

# Prepare data for the radar plot
radar_data <- rbind(rep(1, 5), rep(0, 5), data_normalized)

# Plot the radar chart
radarchart(
  radar_data,
  axistype = 1,
  pcol = viridis(1),
  pfcol = scales::alpha(viridis(1), 0.5),
  plwd = 4,
  plty = 1,
  cglcol = "grey",
  cglty = 1,
  axislabcol = "grey",
  caxislabels = seq(0, 1, 0.2),
  cglwd = 0.8,
  vlcex = 1.2,
  title = "Association Strength with Edible"
)


```

# Feature Engineering:

Feature engineering is the process of creating new features or modifying existing ones to improve the performance of machine learning models. In the context of the mushrooms dataset, a new feature `CapShape_Color` is created by combining `CapShape` and `CapColor`. This feature engineering approach can be beneficial for several reasons:

## Advantages of Feature Engineering:

- **Capturing Interactions Between Variables**: By combining `CapShape` and `CapColor`, the new feature can capture interactions between these two variables, which might provide more predictive power than considering each attribute separately.
- **Increasing Model Complexity**: Adding interaction features like `CapShape_Color` can increase the model's complexity, allowing it to capture more intricate patterns in the data, which can be particularly beneficial for decision tree-based models.
- **Improving Model Performance**: In some cases, models can perform better when they have access to features that represent combined attributes, leading to better classification accuracy or other performance metrics.
- **Feature Selection and Dimensionality Reduction**: Creating combined features can sometimes help in reducing the dimensionality of the feature space if it leads to fewer, more informative features being selected by the model during the feature selection process.

## Implementation:

- The new feature `CapShape_Color` is created by concatenating `CapShape` and `CapColor` using the `paste()` function in R.
- For example, if a mushroom has a cap shape of "Convex" and a cap color of "Brown", the new feature will be "Convex_Brown".

## Impact on the Model:

- **Decision Trees**: Decision trees can split on this combined feature, capturing complex patterns where specific combinations of cap shape and color might correlate with the edibility of the mushroom.
- **Random Forests**: Random forests, which are an ensemble of decision trees, can also benefit from this additional complexity, potentially improving the overall predictive performance by considering these interactions.



```{r feature engineering,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Feature Engineering
# Create a new feature combining CapShape and CapColor
mushrooms <- mushrooms %>%
  mutate(CapShape_Color = paste(CapShape, CapColor, sep = "_"))

```


```{r prepare data before modelling,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Convert categorical variables to factors
mushrooms <- mushrooms %>%
  mutate(across(where(is.character), as.factor))

# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(mushrooms$Edible, p = 0.7, list = FALSE)
trainData <- mushrooms[trainIndex, ]
testData <- mushrooms[-trainIndex, ]

```

# Task 1:

# Data Modelling:

## Decision Tree Classifier:

A decision tree is a supervised machine learning algorithm used for classification and regression tasks. It partitions data into subsets based on the value of input features, creating a tree-like model of decisions and their possible consequences. Each internal node represents a test on an attribute (e.g., "gills color?"), each branch represents the outcome of the test (e.g., "white"), and each leaf node represents a class label (e.g., "edible" or "poisonous").

**Information Gain Formula**

The key formula in decision trees is the information gain (IG) or entropy reduction, which determines the best feature to split on at each node:

$$IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)$$

Where:
- $H(S)$ is the entropy of the set $S$: $H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)$, with $p_i$ as the proportion of instances in class $i$ out of $c$ classes.
- $Values(A)$ is the set of all possible values for attribute $A$.
- $S_v$ is the subset of $S$ for which attribute $A$ has value $v$.
- $|S|$ is the number of elements in set $S$.

The attribute with the highest information gain is chosen for the decision node, as it provides the most information about the classes.


```{r decision tree model,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
library(caret)
library(rpart.plot)


# Train the model
model_dt <- train(Edible ~ ., data = trainData, method = "rpart")

# Predict on the test data
predictions <- predict(model_dt, testData)

# Evaluate the model
conf_matrix_dt <- confusionMatrix(predictions, testData$Edible)
print(conf_matrix_dt)


```


**Confusion Matrix:**

The confusion matrix breaks down the model's predictions into four categories. Out of the test set observations, the model correctly classified 1,148 edible mushrooms as edible and 1,138 poisonous mushrooms as poisonous. However, it made some errors – misclassifying 114 edible mushrooms as poisonous (false negatives) and 36 poisonous mushrooms as edible (false positives).

**Accuracy and Reliability:**

- The overall accuracy achieved by the model was 93.84%, meaning it correctly classified 93.84% of the mushrooms in the test set as either edible or poisonous.
- The 95% confidence interval for accuracy ranged from 92.81% to 94.76%, suggesting a high reliability in the reported accuracy estimate. It can be 95% confident that the true accuracy lies within this range.
- The p-value for testing if the model's accuracy is better than a random guess was less than 2.2e-16, an extremely low value indicating strong statistical significance.

**Performance Metrics:**

- The Kappa statistic was 0.877, indicating very good agreement between the model's predictions and the actual classifications, even after accounting for agreement by chance.
- The model's sensitivity or recall for the edible class was 90.97%, meaning it correctly identified 90.97% of the truly edible mushrooms.
- The specificity or recall for the poisonous class was 96.93%, meaning it correctly identified 96.93% of the truly poisonous mushrooms.
- The positive predictive value or precision for the edible class was 96.96%, indicating that when the model predicted a mushroom as edible, it was correct 96.96% of the time.

**Class Balance:**

- The balanced accuracy, calculated as the average of sensitivity and specificity, was 93.95%. This suggests that the model's performance was well-balanced across both the edible and poisonous classes.
- However, McNemar's test p-value was 3.236e-10, a low value indicating a statistically significant difference in the misclassification rates between the two classes. 


```{r decision tree visualisation,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}

# Visualize decision tree with enhanced appearance
prp(model_dt$finalModel,
    type = 4,              # Tree type: 4 for compact
    extra = 101,           # Additional graphical parameters
    branch = 0.5,          # Branch line width
    varlen = 0,            # Variable name abbreviation
    faclen = 0,            # Factor level abbreviation
    fallen = TRUE,         # Show node labels
    tweak = 1,             # Line tweak
    under = TRUE,          # Plot branches under the node bars
    compress = TRUE,       # Reduce white space
    box.col = c("#FF9999", "#99CCFF"),  # Node colors
    split.box.col = "white",             # Split box color
    split.cex = 0.5,       # Split text size
    split.prefix = "",     # Split prefix text
    split.suffix = "",     # Split suffix text   # Split box border color
    split.round = 0.4,      # Split box rounding
    digits = 2,            # Number of digits in splits
    shadow.col = "gray",   # Shadow color
    branch.lwd = 2,        # Branch line width
)


```

The decision tree model is structured around two key features: `OdorNone` and `OdorAnise`. 

**Root Node Split:**
The root node splits the data based on whether the mushroom has no odor (`OdorNone = 1`) or has an odor (`OdorNone = 0`). This split separates:
- Mushrooms with no odor (2,468 observations, 43% of the data, mostly edible)
- Mushrooms with an odor (3,220 observations, 57% of the data, mostly poisonous)

**Edible Mushroom Path:**
For mushrooms with no odor (`OdorNone = 1`), the model predicts them as edible. This path contains 2,384 edible mushrooms and 84 poisonous mushrooms, with a high accuracy of identifying edible mushrooms.

**Poisonous Mushroom Path:**
Mushrooms with an odor (`OdorNone = 0`) are further split based on whether the odor is anise (`OdorAnise = 1`) or not (`OdorAnise = 0`).
- Mushrooms with anise odor (276 observations, all edible) are predicted as edible.
- Mushrooms with non-anise odors (2,944 observations, mostly poisonous) are predicted as poisonous, with 2,658 correctly classified as poisonous.

**Interpretability:**
The decision tree provides clear and interpretable rules for classification based on odor features.It highlights `OdorNone` and `OdorAnise` as the most important predictors of edibility.

**Overall Interpretation:**
The decision tree model demonstrated high accuracy of 93.84% in classifying mushrooms as either edible or poisonous based on their features. Its Kappa statistic of 0.877 and balanced accuracy of 93.95% further highlighted strong performance across both classes, not favoring one over the other. While the model was marginally better at identifying poisonous mushrooms (96.93% specificity) than edible ones (90.97% sensitivity), it was highly precise at 96.96% when predicting a mushroom as edible. These metrics emphasise the model's robustness and reliability for this critical classification task.


## Decision Tree Classifier using C5.0 Algorithm:

The C5.0 algorithm is an advanced version of the ID3 and C4.5 algorithms for building decision trees. It incorporates several enhancements over its predecessors, making it faster, more memory efficient, and capable of generating smaller decision trees. Key features include:

1. **Information Gain Ratio**: Uses a normalized version of information gain to reduce bias towards attributes with many outcomes.

2. **Boosting**: Combines multiple trees to improve accuracy.

3. **Variable Misclassification Costs**: Allows specifying higher costs for certain types of misclassifications.

4. **Winnowing**: Automatically removes attributes that do not contribute to the model.

5. **Pruning**: Reduces tree size and complexity to prevent overfitting.

```{r c5.0,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
library(C50)
library(caret)
library(partykit)

# Train the C5.0 model
model_C50 <- C5.0(Edible ~ ., data = trainData)

# Predict on the test data
predictions_C50 <- predict(model_C50, testData)

# Evaluate the model
conf_matrix_C50 <- confusionMatrix(predictions_C50, testData$Edible)
print(conf_matrix_C50)

```

```{r, fig.width=12, fig.height=10 ,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Plot the decision tree using C50's plot method with adjusted parameters
plot(model_C50, main = "Decision Tree (C5.0)", fallen.leaves = TRUE, type = "extended", control = C5.0Control(bands = 0.1))
```

## Random Forest Classifier:

A random forest is an ensemble learning method for classification, regression, and other tasks. It constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests averages multiple decision trees' avoiding overfitting to their training set.


```{r approach 4,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Build a predictive model using randomForest
mushrooms <- read.csv('mushrooms.csv')

# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(mushrooms$Edible, p = .8, 
                                  list = FALSE, 
                                  times = 1)
trainData <- mushrooms[ trainIndex,]
testData  <- mushrooms[-trainIndex,]

# Ensure that the levels of the factors are the same
trainData$Edible <- factor(trainData$Edible, levels = c('Edible', 'Poisonous'))
testData$Edible <- factor(testData$Edible, levels = c('Edible', 'Poisonous'))
model_rf <- randomForest(Edible ~ ., data = trainData, ntree = 100)

# Predict on the test data
predictions_rf <- predict(model_rf, testData)

# Evaluate the model
conf_matrix_rf <- confusionMatrix(predictions_rf, testData$Edible)
print(conf_matrix_rf)

```

**Confusion Matrix: **
The confusion matrix shows the model correctly classified 1,262 edible mushrooms and 1,155 poisonous mushrooms. It misclassified only 19 poisonous mushrooms as edible, with no edible mushrooms misclassified as poisonous.

**Accuracy:**
- The model achieved an accuracy of 99.22%, correctly classifying over 99 out of 100 mushrooms.
- The 95% confidence interval was 98.78% to 99.53%, indicating high reliability.

**Performance Metrics:**
- The Kappa value of 0.9844 indicates near-perfect agreement between predictions and actual classes.
- Sensitivity for the edible class was 100%, meaning the model correctly identified all edible mushrooms.
- Specificity for the poisonous class was 98.38%, correctly identifying over 98% of poisonous mushrooms.
- Positive predictive value was 98.52%, so when predicted edible, the model was correct over 98% of the time.
- Negative predictive value was 100%, meaning all predictions of poisonous were correct.

**Class Balance: **
- The balanced accuracy of 99.19% suggests exceptional performance across both classes.
- However, McNemar's test p-value of 3.636e-05 indicates a statistically significant difference in misclassification rates between classes.

**Overall:**
The random forest model demonstrated near-perfect accuracy of 99.22% in classifying mushrooms. Its balanced accuracy of 99.19% and perfect sensitivity highlighted outstanding performance across classes. With high specificity of 98.38% and precision of 98.52%, the model was highly reliable in identifying both edible and poisonous mushrooms.

**Conclusion:**
The random forest model provides a highly robust and reliable solution for predicting mushroom edibility. Its near-perfect accuracy, sensitivity, and specificity make it an excellent tool for this critical task. However, the imbalance in misclassification rates suggests potential for further improvement.

```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}

# Plot the importance of variables
varImpPlot(model_rf, main = "Variable Importance Plot using randomForest")
```

 
## Random Forest Classifier using Hyperparameter tuning:

Hyperparameter tuning optimizes model parameters that control algorithm behavior but are not learned during training. Here, grid search with 10-fold cross-validation is used and it tunes the `mtry` parameter of a random forest, which determines the number of features considered at each split. By testing `mtry` values from 1 to 5, the model finds the value that best balances feature consideration and tree diversity for the mushroom dataset. The best `mtry` is used to train a final model, which is evaluated on test data using a confusion matrix. This process tailors the model to the dataset's characteristics, potentially improving accuracy, sensitivity (critical for not misclassifying poisonous mushrooms as edible), and computational efficiency. The best `mtry` value can also provide insights into the dataset's feature importance. 


```{r random forest,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Define the control using a random search
control <- trainControl(method = "cv", number = 10, search = "grid")

# Define the grid of hyperparameters
tunegrid <- expand.grid(.mtry = c(1:5))

# Train the model using random forest and grid search
set.seed(123)
rf_gridsearch <- train(
  Edible ~ ., 
  data = trainData, 
  method = "rf", 
  metric = "Accuracy", 
  tuneGrid = tunegrid, 
  trControl = control
)

# Print the best parameters
print(rf_gridsearch$bestTune)

# Predict on the test data using the best model
predictions_rf_grid <- predict(rf_gridsearch, testData)

# Evaluate the model
conf_matrix_rf <- confusionMatrix(predictions_rf_grid, testData$Edible)
print(conf_matrix_rf)

```
The hyperparameter-tuned Random Forest model demonstrates performance in classifying mushroom edibility. The best tune is achieved at 5, achieving 97.5% accuracy in correctly identifying 97 out of 100 mushrooms. With a Kappa value of 0.9499 indicating agreement between predicted and actual classes, the model exhibits balanced performance across edible (97.07% sensitivity) and poisonous (97.96% specificity) mushrooms. Its balanced accuracy of 97.51% underscores performance across classes. Moreover, the model's precision of 98.08% for edible predictions and 96.88% negative predictive value for poisonous predictions highlight reliability. The insignificant difference in error rates between classes (Mcnemar's p-value: 0.1244) adds to robustness.


# Task 2

# Cross Validating the models:

**1. K-fold Cross-Validation:**

K-fold cross-validation is a model validation technique where the dataset is divided into k subsets, or folds; the model is trained on k-1 folds and tested on the remaining fold, repeating this process k times with each fold serving as the test set once. For example, in 5-fold cross-validation of a mushroom dataset with 8,124 instances, the data is split into 5 subsets of about 1,625 instances each; the model is trained on 4 folds (6,500 instances) and tested on the fifth (1,624 instances), repeating this for all folds.

Number of instances in each fold:
$$n_{fold} = \left\lfloor\frac{n}{k}\right\rfloor$$
where $n$ is the total number of instances and $k$ is the number of folds.
Performance metric (e.g., accuracy) for k-fold CV:
$$\text{CV}k = \frac{1}{k}\sum{i=1}^{k}\text{Accuracy}_i$$
where $\text{Accuracy}_i$ is the accuracy on the $i$-th fold.

Example:
- Total instances: $n = 8,124$
- Folds: $k = 5$
- Instances per fold: $n_{fold} = \lfloor\frac{8,124}{5}\rfloor = 1,624$
- 5-fold CV accuracy: $\text{CV}_5 = \frac{1}{5}(0.9945 + 0.9945 + 0.9945 + 0.9945 + 0.9945) = 0.9945$

**2. Bootstrapping:** 

Bootstrapping is a resampling method where multiple datasets of the same size as the original are created by randomly sampling with replacement, allowing instances to appear multiple times in a bootstrap sample. For instance, in the mushroom dataset with 8,124 instances, a bootstrap sample might also contain 8,124 instances, but some mushrooms may appear multiple times while others may not appear at all; this process is repeated to create multiple bootstrap samples for training and evaluating the model.

Number of instances in a bootstrap sample:
$$n_{boot} = n$$
where $n$ is the total number of instances.
Probability of an instance being selected in a bootstrap sample:
$$P(\text{select}) = 1 - \left(1 - \frac{1}{n}\right)^n \approx 1 - e^{-1} \approx 0.632$$
Bootstrap performance metric (e.g., accuracy):
$$\text{Accuracy}{boot} = \frac{1}{B}\sum{i=1}^{B}\text{Accuracy}_i$$
where $B$ is the number of bootstrap samples and $\text{Accuracy}_i$ is the accuracy on the $i$-th bootstrap sample.

Example:

- Total instances: $n = 8,124$
- Bootstrap sample size: $n_{boot} = 8,124$
- Probability of an instance in a bootstrap sample: $P(\text{select}) \approx 0.632$
- Bootstrap accuracy (B=100): $\text{Accuracy}_{boot} = \frac{1}{100}\sum_{i=1}^{100}\text{Accuracy}_i = 0.9945$

### Decision Tree Validation - Using K-fold cross validation:

```{r k fold cross validation decision tree,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Set up k-fold cross-validation
set.seed(123)  # For reproducibility
control_kfold <- trainControl(method = "cv", number = 10)
# Train decision tree model using k-fold cross-validation
set.seed(123)
model_dt_kfold <- train(Edible ~ ., data = trainData, method = "rpart", trControl = control_kfold)

# Predict on the test data
predictions_dt_kfold <- predict(model_dt_kfold, testData)
# Evaluate the models
conf_matrix_dt_kfold <- confusionMatrix(predictions_dt_kfold, testData$Edible)
# Print confusion matrices
print(conf_matrix_dt_kfold)

```
### Decision Tree Validation - Using Bootstrapping:

```{r bootstrapping decision tree,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Set up bootstrapping
set.seed(123)  # For reproducibility
control_boot <- trainControl(method = "boot", number = 50)

# Train decision tree model using bootstrapping
set.seed(123)
model_dt_boot <- train(Edible ~ ., data = trainData, method = "rpart", trControl = control_boot)
 
# Predict on the test data
predictions_dt_boot <- predict(model_dt_boot, testData)

# Evaluate the models
conf_matrix_dt_boot <- confusionMatrix(predictions_dt_boot, testData$Edible)

# Print confusion matrices
print(conf_matrix_dt_boot)

```


```{r comparison table decision tree,warning=FALSE,echo=FALSE,include=TRUE,fig.align='center',message=FALSE}

# Extracting performance metrics for each model
performance_metrics <- data.frame(
  Model = c("Decision Tree", "C5.0 Algorithm", "Decision Tree (k-fold CV)", "Decision Tree (Bootstrapping)"),
  Accuracy = c(
    conf_matrix_dt$overall['Accuracy'],
    conf_matrix_C50$overall['Accuracy'],
    conf_matrix_dt_kfold$overall['Accuracy'],
    conf_matrix_dt_boot$overall['Accuracy']
  ),
  Kappa = c(
    conf_matrix_dt$overall['Kappa'],
    conf_matrix_C50$overall['Kappa'],
    conf_matrix_dt_kfold$overall['Kappa'],
    conf_matrix_dt_boot$overall['Kappa']
  ),
  Sensitivity = c(
    conf_matrix_dt$byClass['Sensitivity'],
    conf_matrix_C50$byClass['Sensitivity'],
    conf_matrix_dt_kfold$byClass['Sensitivity'],
    conf_matrix_dt_boot$byClass['Sensitivity']
  ),
  Specificity = c(
    conf_matrix_dt$byClass['Specificity'],
    conf_matrix_C50$byClass['Specificity'],
    conf_matrix_dt_kfold$byClass['Specificity'],
    conf_matrix_dt_boot$byClass['Specificity']
  ),
  PPV = c(
    conf_matrix_dt$byClass['Pos Pred Value'],
    conf_matrix_C50$byClass['Pos Pred Value'],
    conf_matrix_dt_kfold$byClass['Pos Pred Value'],
    conf_matrix_dt_boot$byClass['Pos Pred Value']
  ),
  NPV = c(
    conf_matrix_dt$byClass['Neg Pred Value'],
    conf_matrix_C50$byClass['Neg Pred Value'],
    conf_matrix_dt_kfold$byClass['Neg Pred Value'],
    conf_matrix_dt_boot$byClass['Neg Pred Value']
  ),
  Balanced_Accuracy = c(
    conf_matrix_dt$byClass['Balanced Accuracy'],
    conf_matrix_C50$byClass['Balanced Accuracy'],
    conf_matrix_dt_kfold$byClass['Balanced Accuracy'],
    conf_matrix_dt_boot$byClass['Balanced Accuracy']
  )
)


options(knitr.kable.NA = "")
kable(performance_metrics, format = "latex", booktabs = TRUE, linesep = "", caption = "Performance Metrics for Decision Tree Classifiers") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 7, 
                full_width = FALSE) %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2:8, width = "2cm") %>%
  kable_styling(latex_options = "striped", position = "center", stripe_color = "gray!15")
```

### Model Comparison for Mushroom Classification using Decision Tree:

Four decision tree models were evaluated for classifying mushrooms as edible or poisonous: a basic Decision Tree (rpart), a C5.0 algorithm, a Decision Tree with k-fold cross-validation, and a Decision Tree with bootstrapping. The performance of each model was assessed using confusion matrices and key performance metrics.

**Basic Decision Tree Performance**

The basic Decision Tree model correctly classified 2,286 out of 2,436 mushrooms, resulting in an accuracy of 93.84%. The model's Kappa statistic of 0.877 indicates strong agreement beyond chance. Out of 1,262 edible mushrooms, the model correctly identified 1,148 (90.97% sensitivity). For 1,174 poisonous mushrooms, the model correctly identified 1,138 (96.93% specificity). The PPV of 96.96% means that when the model predicts a mushroom as edible, it is correct 96.96% of the time. The NPV of 90.89% means that when it predicts a mushroom as poisonous, it is correct 90.89% of the time.

**C5.0 Algorithm Performance**

The C5.0 Algorithm correctly classified 2,415 out of 2,436 mushrooms, achieving an accuracy of 99.14%. The Kappa statistic of 0.9827 indicates almost perfect agreement. The model correctly identified all 1,262 edible mushrooms (100% sensitivity) and 1,153 out of 1,174 poisonous mushrooms (98.21% specificity). When the C5.0 model predicts a mushroom as edible, it is correct 98.36% of the time (PPV). Notably, when it predicts a mushroom as poisonous, it is always correct (100% NPV).

**Decision Tree with Cross-Validation and Bootstrapping**

Both the Decision Tree after k-fold cross-validation and the Decision Tree with bootstrapping showed identical performance to the basic Decision Tree model. This consistency across 2,286 correct classifications out of 2,436 mushrooms (93.84% accuracy) suggests that the original model's performance is stable across different data subsets and resampling techniques.

**Model Selection**

The C5.0 Algorithm outperforms other models by correctly classifying 129 more mushrooms than the decision tree models. Its perfect sensitivity (100% vs. 90.97%) means it identifies all edible mushrooms correctly, reducing the risk of consuming poisonous mushrooms. The C5.0 model's 100% NPV is critical, as it ensures that when a mushroom is classified as poisonous, it is always poisonous.

**Approach and Results**

K-fold cross-validation and bootstrapping validated the models' performance consistency. The C5.0 model's superior metrics, including a 5.3 percentage point higher accuracy and perfect sensitivity and NPV, make it the clear choice for safe mushroom classification. The Mcnemar's test p-values (1.275e-05 for C5.0, 3.236e-10 for others) confirm statistically significant performance differences.


```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Load necessary libraries
library(caret)
library(C50)
library(rpart)
library(stats)

# Load the data
mushrooms <- read.csv('mushrooms.csv')

# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(mushrooms$Edible, p = .8, 
                                  list = FALSE, 
                                  times = 1)
trainData <- mushrooms[ trainIndex,]
testData  <- mushrooms[-trainIndex,]

# Ensure that the levels of the factors are the same
trainData$Edible <- factor(trainData$Edible, levels = c('Edible', 'Poisonous'))
testData$Edible <- factor(testData$Edible, levels = c('Edible', 'Poisonous'))

# Train the models
model_dt <- train(Edible ~ ., data = trainData, method = 'rpart')
predictions_dt <- predict(model_dt, testData)
predictions_dt <- factor(predictions_dt, levels = c('Edible', 'Poisonous'))
conf_matrix_dt <- confusionMatrix(predictions_dt, testData$Edible)


model_C50 <- C5.0(Edible ~ ., data = trainData)
predictions_C50 <- predict(model_C50, testData)
predictions_C50 <- factor(predictions_C50, levels = c('Edible', 'Poisonous'))
conf_matrix_C50 <- confusionMatrix(predictions_C50, testData$Edible)

set.seed(123)
control_kfold <- trainControl(method = 'cv', number = 10)
model_dt_kfold <- train(Edible ~ ., data = trainData, method = 'rpart', trControl = control_kfold)
predictions_dt_kfold <- predict(model_dt_kfold, testData)
predictions_dt_kfold <- factor(predictions_dt_kfold, levels = c('Edible', 'Poisonous'))
conf_matrix_dt_kfold <- confusionMatrix(predictions_dt_kfold, testData$Edible)


control_boot <- trainControl(method = 'boot', number = 50)
model_dt_boot <- train(Edible ~ ., data = trainData, method = 'rpart', trControl = control_boot)
predictions_dt_boot <- predict(model_dt_boot, testData)
predictions_dt_boot <- factor(predictions_dt_boot, levels = c('Edible', 'Poisonous'))
conf_matrix_dt_boot <- confusionMatrix(predictions_dt_boot, testData$Edible)


predictions_list <- list(dt = predictions_dt, C50 = predictions_C50, dt_kfold = predictions_dt_kfold, dt_boot = predictions_dt_boot)

# Convert predictions to numeric for statistical tests
predictions_numeric <- lapply(predictions_list, function(x) as.numeric(as.factor(x)))

# Wilcoxon test
wilcox_test_results <- wilcox.test(predictions_numeric$dt, predictions_numeric$C50, paired = TRUE)


# Paired t-test
paired_t_test_results <- t.test(predictions_numeric$dt, predictions_numeric$C50, paired = TRUE)

```


```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
predictions_list <- list(dt = predictions_dt, C50 = predictions_C50, dt_kfold = predictions_dt_kfold, dt_boot = predictions_dt_boot)

# Convert predictions to numeric for statistical tests
predictions_numeric <- lapply(predictions_list, function(x) as.numeric(as.factor(x)))

wilcox_test_results_list <- list()
paired_t_test_results_list <- list()

model_names <- names(predictions_numeric)
for (i in 1:(length(model_names) - 1)) {
  for (j in (i + 1):length(model_names)) {
    model1 <- model_names[i]
    model2 <- model_names[j]
    # Wilcoxon test
    wilcox_test_results <- wilcox.test(predictions_numeric[[model1]], predictions_numeric[[model2]], paired = TRUE)
    wilcox_test_results_list[[paste(model1, model2, sep = '_vs_')]] <- wilcox_test_results
    # Paired t-test
    paired_t_test_results <- t.test(predictions_numeric[[model1]], predictions_numeric[[model2]], paired = TRUE)
    paired_t_test_results_list[[paste(model1, model2, sep = '_vs_')]] <- paired_t_test_results
  }
}

```



```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Extracting results into a data frame
results <- data.frame(
  Model_Pair = character(),
  Wilcoxon_V = numeric(),
  Wilcoxon_p_value = numeric(),
  Paired_t_statistic = numeric(),
  Paired_t_df = numeric(),
  Paired_t_p_value = numeric(),
  Paired_t_mean_diff = numeric(),
  stringsAsFactors = FALSE
)

for (pair in names(wilcox_test_results_list)) {
  wilcox_test <- wilcox_test_results_list[[pair]]
  paired_t_test <- paired_t_test_results_list[[pair]]
  
  results <- rbind(results, data.frame(
    Model_Pair = pair,
    Wilcoxon_V = wilcox_test$statistic,
    Wilcoxon_p_value = wilcox_test$p.value,
    Paired_t_statistic = paired_t_test$statistic,
    Paired_t_df = paired_t_test$parameter,
    Paired_t_p_value = paired_t_test$p.value,
    Paired_t_mean_diff = paired_t_test$estimate
  ))
}

# Print the results table using kable
kable(results, format = "latex", booktabs = TRUE, linesep = "", caption = "Wilcoxon and Paired t-test results for Decision Tree Classifiers") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 7, 
                full_width = FALSE) %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2:8, width = "2cm") %>%
  kable_styling(latex_options = "striped", position = "center", stripe_color = "gray!15")

```

# Statistical Significance for Decision Trees:

The Wilcoxon signed-rank test and paired t-test were used to compare four mushroom classification models: a basic Decision Tree (dt), a C5.0 algorithm (C50), a Decision Tree with k-fold cross-validation (dt_kfold), and a Decision Tree with bootstrapping (dt_boot).

Both statistical tests revealed that the C5.0 model's predictions are significantly different from all decision tree models (dt, dt_kfold, dt_boot). The p-values for these comparisons were extremely low (1.803e-14 for Wilcoxon and 1.043e-14 for t-test), indicating strong statistical significance.

In contrast, the predictions from the basic decision tree (dt), the k-fold cross-validated decision tree (dt_kfold), and the bootstrap decision tree (dt_boot) were identical. This identity resulted in the inability to perform the statistical tests for these comparisons.

**Significance of the C5.0 Model**

The C5.0 model is statistically significant because its predictions differ significantly from all decision tree variations. The mean difference in predictions between C5.0 and the decision trees is approximately 0.042, with a 95% confidence interval not including zero. This suggests that the C5.0 model's superior performance, as seen in its higher accuracy, sensitivity, and specificity, is a genuine difference rather than random chance.

### Random Forest Classfication Validation- Using K-fold cross validation:


```{r k fold random forest,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Train random forest model using k-fold cross-validation
control_kfold <- trainControl(method = "cv", number = 5)
set.seed(123)
model_rf_kfold <- train(Edible ~ ., data = trainData, method = "rf", trControl = control_kfold)

predictions_rf_kfold <- predict(model_rf_kfold, testData)
conf_matrix_rf_kfold <- confusionMatrix(predictions_rf_kfold, testData$Edible)
print(conf_matrix_rf_kfold)
```

### Random Forest Classfication Validation- Using Bootstrapping:

```{r bootstrapping random forest,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
control_boot <- trainControl(method = "boot", number = 25)
set.seed(123)
model_rf_boot <- train(Edible ~ ., data = trainData, method = "rf", trControl = control_boot)

predictions_rf_boot <- predict(model_rf_boot, testData)

conf_matrix_rf_boot <- confusionMatrix(predictions_rf_boot, testData$Edible)
print(conf_matrix_rf_boot)
```

```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}

# Load necessary library
library(knitr)

# Create a data frame with the performance metrics
performance_metrics <- data.frame(
  Model = c("Random Forest", "Random Forest (Hyperparameter)", "Random Forest (k-fold CV)", "Random Forest (Bootstrapping)"),
  Accuracy = c(0.9686, 0.9914, 0.9945, 0.9945),
  Kappa = c(0.9372, 0.9827, 0.9889, 0.9889),
  Sensitivity = c(0.9560, 1.0000, 1.0000, 1.0000),
  Specificity = c(0.9821, 0.9821, 0.9885, 0.9885),
  PPV = c(0.9829, 0.9836, 0.9894, 0.9894),
  NPV = c(0.9541, 1.0000, 1.0000, 1.0000),
  Balanced_Accuracy = c(0.9691, 0.9911, 0.9943, 0.9943),
  McNemar_p_value = c(0.002066, 0.000512, 0.007661, 0.007661)
)

kable(performance_metrics, format = "latex", booktabs = TRUE, linesep = "",caption = "Performance metrics for Random Forest Classifiers") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 7, 
                full_width = FALSE) %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2:8, width = "2cm") %>%
  kable_styling(latex_options = "striped", position = "center", stripe_color = "gray!15")

```
### Model Comparison for Mushroom Classification using Random Forest Classifiers:

Four Random Forest classifiers were evaluated for mushroom classification: standard Random Forest (RF), Hyperparameter-tuned RF (Hyper RF), RF with k-fold cross-validation (RF k-fold), and RF with bootstrapping (RF Bootstrap). All models showed high performance, but with notable differences. Standard RF achieved 96.86% accuracy and 96.91% balanced accuracy. The other three models significantly outperformed it, with accuracies and balanced accuracies above 99%.

**Top Performers:**
RF k-fold and RF Bootstrap models were identical in performance, leading the pack with 99.45% accuracy and 99.43% balanced accuracy. Their perfect sensitivity (100%) means they correctly identified all edible mushrooms, and their 100% NPV ensures no poisonous mushrooms were misclassified as edible - critical for user safety. Hyper RF closely followed with 99.14% accuracy and 99.11% balanced accuracy, also achieving perfect sensitivity and NPV.

**Statistical Significance:**
McNemar's Test p-values (all <0.01) indicate statistically significant performance differences between the models. Notably, Hyper RF had the lowest p-value (0.000512), suggesting its improvements over standard RF are the most significant. This aligns with findings from Wilcoxon and paired t-tests, which showed Hyper RF having the highest mean performance improvement (2.28%) over standard RF.

For mushroom classification, RF k-fold and RF Bootstrap are the top choices, with Hyper RF as a strong alternative. All three significantly outperform standard RF, offering near-perfect accuracy and, most importantly, perfect safety in not misclassifying poisonous mushrooms. The choice among these three may depend on computational resources and the specific needs of the application.


```{r,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Convert predictions to numeric values

predictions_rf_num <- as.numeric(predictions_rf)
predictions_rf_grid_num <- as.numeric(predictions_rf_grid)
predictions_rf_kfold_num <- as.numeric(predictions_rf_kfold)
predictions_rf_boot_num <- as.numeric(predictions_rf_boot)

# Check the lengths of the prediction vectors
length_rf <- length(predictions_rf_num)
length_rf_grid <- length(predictions_rf_grid_num)
length_rf_kfold <- length(predictions_rf_kfold_num)
length_rf_boot <- length(predictions_rf_boot_num)

  # Wilcoxon test between Model 1 and Hypertuned Model 2
  wilcox_test_1_2 <- wilcox.test(predictions_rf_num, predictions_rf_grid_num, paired = TRUE)

  # Wilcoxon test between Model 1 and Model 3
  wilcox_test_1_3 <- wilcox.test(predictions_rf_num, predictions_rf_kfold_num, paired = TRUE)

  # Wilcoxon test between Model 1 and Model 4
  wilcox_test_1_4 <- wilcox.test(predictions_rf_num, predictions_rf_boot_num, paired = TRUE)

  
  # Paired t-test between Model 1 and Hypertuned Model 2
  t_test_1_2 <- t.test(predictions_rf_num, predictions_rf_grid_num, paired = TRUE)

  
  # Paired t-test between Model 1 and Model 3
  t_test_1_3 <- t.test(predictions_rf_num, predictions_rf_kfold_num, paired = TRUE)

  # Paired t-test between Model 1 and Model 4
  t_test_1_4 <- t.test(predictions_rf_num, predictions_rf_boot_num, paired = TRUE)


```


```{r,warning=FALSE,echo=FALSE,include=TRUE,fig.width=8,message=FALSE}
# Load necessary libraries
library(knitr)
library(kableExtra)

# Assuming the Wilcoxon test and paired t-test results are already defined:
# wilcox_test_1_2, wilcox_test_1_3, wilcox_test_1_4
# t_test_1_2, t_test_1_3, t_test_1_4

# Create the results data frame
results <- data.frame(
  Model_Pair = c("RF_vs_Hypertuned_RF", "RF_vs_RF_kfold", "RF_vs_RF_boot"),
  Wilcoxon_V = c(wilcox_test_1_2$statistic, wilcox_test_1_3$statistic, wilcox_test_1_4$statistic),
  Wilcoxon_p_value = c(wilcox_test_1_2$p.value, wilcox_test_1_3$p.value, wilcox_test_1_4$p.value),
  Paired_t_statistic = c(t_test_1_2$statistic, t_test_1_3$statistic, t_test_1_4$statistic),
  Paired_t_df = c(t_test_1_2$parameter, t_test_1_3$parameter, t_test_1_4$parameter),
  Paired_t_p_value = c(t_test_1_2$p.value, t_test_1_3$p.value, t_test_1_4$p.value),
  Paired_t_mean_diff = c(t_test_1_2$estimate, t_test_1_3$estimate, t_test_1_4$estimate)
)

# Print the results table using kable
kable(results, format = "latex", booktabs = TRUE, linesep = "", caption = "Wilcoxon and Paired t-test results for Random Forest Classifiers") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 7, 
                full_width = FALSE) %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2:8, width = "2cm") %>%
  kable_styling(latex_options = "striped", position = "center", stripe_color = "gray!15")

```

# Statistical Significance for Random Forest Classifiers:

This study evaluates four Random Forest (RF) models for mushroom classification: standard RF, Hypertuned RF, RF with k-fold cross-validation (RF_kfold), and RF with bootstrapping (RF_boot). Statistical significance was determined using Wilcoxon signed-rank tests and paired t-tests, both of which assess whether the differences in model performances are due to chance or represent genuine improvements.

**Hypertuned RF Performance:**
Hypertuned RF emerged as the best model. It outperformed the standard RF with extreme statistical significance - both Wilcoxon and paired t-test p-values were 0, indicating virtually no chance that the performance difference is random. The mean improvement was 2.28%, suggesting that on average, Hypertuned RF's predictions (likely accuracy or another key metric) were 2.28 percentage points better than standard RF. This improvement, while seemingly small, can be substantial in high-stakes scenarios like mushroom toxicity classification.

**RF_kfold and RF_boot Performance:**
Both RF_kfold and RF_boot showed identical performance improvements over standard RF. Their Wilcoxon p-value (8e-07) and paired t-test p-value (7e-07) indicate high statistical significance, with very low probabilities that these improvements are due to chance. Both models improved performance by an average of 1.97 percentage points. While not as high as Hypertuned RF, this is still a notable and statistically significant enhancement.

In conclusion, based solely on the provided statistical data, the Hypertuned RF model appears to be the best, as it shows the largest statistically significant improvement in predictions.

# Model Selection:

```{r, fig.width=12, fig.height=10,,warning=FALSE,echo=FALSE,include=TRUE,message=FALSE}
# Load necessary libraries
library(tidyverse)
library(reshape2)

# Create a data frame with the performance metrics
performance_metrics <- data.frame(
  Model = c("Decision Tree", "C5.0 Algorithm", "Decision Tree (k-fold CV)", "Decision Tree (Bootstrapping)", 
            "Random Forest", "Random Forest (Hyperparameter)", "Random Forest (k-fold CV)", "Random Forest (Bootstrapping)"),
  Accuracy = c(0.9384, 0.9914, 0.9384, 0.9384, 0.9686, 0.9914, 0.9945, 0.9945),
  Kappa = c(0.877, 0.9827, 0.877, 0.877, 0.9372, 0.9827, 0.9889, 0.9889),
  Sensitivity = c(0.9097, 1.0000, 0.9097, 0.9097, 0.9560, 1.0000, 1.0000, 1.0000),
  Specificity = c(0.9693, 0.9821, 0.9693, 0.9693, 0.9821, 0.9821, 0.9885, 0.9885),
  PPV = c(0.9696, 0.9836, 0.9696, 0.9696, 0.9829, 0.9836, 0.9894, 0.9894),
  NPV = c(0.9089, 1.0000, 0.9089, 0.9089, 0.9541, 1.0000, 1.0000, 1.0000),
  Balanced_Accuracy = c(0.9395, 0.9911, 0.9395, 0.9395, 0.9691, 0.9911, 0.9943, 0.9943)
)

# Convert data to long format for easier plotting with ggplot2
performance_metrics_long <- performance_metrics %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")


# Create dot plot for each metric
dot_plot <- ggplot(performance_metrics_long, aes(x = Model, y = Value, color = Metric)) +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal() +
  labs(title = "Performance Metrics Comparison of Different Classifiers", y = "Value") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_color_brewer(palette = "Set1")

print(dot_plot)

```

The analysis of different machine learning models for sorting mushrooms shows strong results. Among the models tested, the Random Forest versions, especially those using k-fold cross-validation and bootstrapping, perform the best. Both models reach a high accuracy of 99.45% and a balanced accuracy of 99.43%, showing they can sort mushrooms correctly most of the time. Importantly, they have perfect sensitivity (100%) and negative predictive value (100%), meaning they never mistake a poisonous mushroom for an edible one, which is vital for user safety.

Statistical tests support the idea that these models are the best. The Wilcoxon and paired t-tests show that the Random Forest models with k-fold cross-validation and bootstrapping are much better than the basic Random Forest model, with very low p-values. These low p-values mean the differences in performance are real, not just by chance. Also, tuning the Random Forest model's settings makes it significantly better than the basic one, showing that optimizing models is important.

While the C5.0 Algorithm does very well with 99.14% accuracy and perfect sensitivity, the best Random Forest models still do better. However, it's worth noting that the C5.0 Algorithm is much better than the basic Decision Tree model in all ways, as shown by the very low p-values.

In summary, for sorting mushrooms, the Random Forest models with k-fold cross-validation or bootstrapping are the best. Their high accuracy, perfect sensitivity, and clear improvements over other models make them the most reliable and effective choices. These models' ability to identify all poisonous mushrooms correctly is especially valuable, ensuring the highest safety for users who rely on these classifications.

# Conclusion:

In this study, machine learning models were evaluated for predicting mushroom edibility, with random forest algorithms utilizing k-fold cross-validation and bootstrapping techniques emerging as the superior classifiers. These models achieved an accuracy of 99.45% and a balanced accuracy of 99.43%, demonstrating optimal performance across both edible and poisonous classes. Critically, they exhibited 100% sensitivity and negative predictive value, ensuring zero false negatives in identifying poisonous mushrooms. This perfect recall is vital in a domain where misclassification poses significant health risks.

Basic decision tree models, while providing interpretable rules based on features like mushroom odor, were outperformed by advanced algorithms. The C5.0 decision tree algorithm demonstrated high performance with 99.14% accuracy and 100% sensitivity, making it a viable candidate, especially in resource-constrained environments. However, the hyperparameter-tuned random forest models marginally outperformed the C5.0 algorithm.

Statistical significance was established through Wilcoxon signed-rank and paired t-tests, confirming that the performance improvements of the C5.0 algorithm and tuned random forest models over basic decision trees are not due to chance. The application of k-fold cross-validation and bootstrapping further enhanced the random forest models, validating their generalizability across different data partitions.

In conclusion, for mushroom edibility prediction, this research recommends random forest models with k-fold cross-validation or bootstrapping. Their near-perfect accuracy, balanced metrics, and flawless poisonous mushroom identification make them the most reliable and safest classifiers. In high-stakes classification tasks, these models offer unmatched performance and safety assurance.
